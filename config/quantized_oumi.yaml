# Quantized Oumi Configuration for CandidateAI
# Lightweight models that work on any laptop

model:
  # Primary model - 8GB Llama 3, quantized to 4GB
  primary: "meta-llama/Llama-3.2-8B-Instruct"
  primary_quantized: True
  primary_bits: 4
  
  # Fallback models for different memory constraints
  fallback:
    medium: "Qwen/Qwen2-7B-Instruct"    # 3.5GB when quantized
    small: "Qwen/Qwen2-1.5B-Instruct"   # 1GB, no quantization needed
    tiny: "microsoft/DialoGPT-medium"   # 500MB, basic chat

# Model loading parameters
model_params:
  primary:
    load_in_4bit: True
    device_map: "auto"
    torch_dtype: "bfloat16"
    trust_remote_code: True
    max_memory: {0: "4GB", "cpu": "8GB"}
    
  fallback:
    medium:
      load_in_4bit: True
      device_map: "auto"
      torch_dtype: "float16"
      
    small:
      # No quantization needed - already lightweight
      device_map: "auto"
      torch_dtype: "float16"
      
    tiny:
      # Basic model, minimal requirements
      device_map: "cpu"

# Memory optimization
optimization:
  attention_implementation: "flash_attention_2"  # If available
  use_cache: False  # Saves memory for inference
  gradient_checkpointing: False  # Not needed for inference
  
# Download caching
cache:
  model_cache_dir: "./models/cache"
  use_offline: True  # Once downloaded, work offline
  verify_checksum: True

# Performance tuning
inference:
  batch_size: 1  # Keeps memory low
  max_new_tokens: 512
  temperature: 0.3
  top_p: 0.9
  repetition_penalty: 1.1
  
# Fallback strategy
fallback_strategy:
  memory_check: True  # Auto-select based on available memory
  cascade: True      # Try smaller models if larger fail
  heuristics: True   # Rule-based fallback if all models fail