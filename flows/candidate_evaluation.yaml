# Hybrid CandidateAI Evaluation Workflow
# Optimized for parallel execution with quantized models

id: candidate-evaluation-hybrid
namespace: candidateai

description: |
  Parallel candidate evaluation using quantized Oumi models and Kestra orchestration
  Optimized for laptop deployment with memory constraints

inputs:
  - name: resume_path
    type: STRING
    description: "Path to resume PDF file"
  - name: job_description
    type: STRING  
    description: "Job description text"
  - name: github_url
    type: STRING
    description: "GitHub repository URL (optional)"
    required: false
  - name: leetcode_username
    type: STRING
    description: "LeetCode username (optional)"
    required: false

variables:
  base_path: "{{ inputs.resume_path | replace('.pdf', '') }}"

tasks:
  # Prepare context and extract data
  - id: prepare-context
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
    inputFiles:
      - pymupdf: requirements.txt
    script: |
      import json
      import os
      import sys
      
      # Install PDF processing
      os.system("pip install pymupdf requests psutil")
      
      try:
          import fitz  # PyMuPDF
          doc = fitz.open("{{ inputs.resume_path }}")
          resume_text = ""
          for page in doc:
              resume_text += page.get_text()
          print(f"Extracted {len(resume_text)} characters from resume")
      except Exception as e:
          print(f"PDF extraction failed: {e}")
          resume_text = "Resume text extraction failed"
      
      # Create context
      context = {
          "resume_path": "{{ inputs.resume_path }}",
          "resume_text": resume_text,
          "job_description": "{{ inputs.job_description }}",
          "github_url": "{{ inputs.github_url }}",
          "leetcode_username": "{{ inputs.leetcode_username }}",
          "base_path": "{{ vars.base_path }}"
      }
      
      # Save context
      with open("{{ vars.base_path }}.context.json", "w") as f:
          json.dump(context, f, indent=2)
      
      print(json.dumps({"status": "prepared", "context_file": "{{ vars.base_path }}.context.json"}))

  # Parallel agent execution
  - id: integrity-agent
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./agents:/agents
    script: |
      import json
      import sys
      sys.path.append("/agents")
      
      from integrity import scan_resume_integrity
      
      # Load context
      with open("{{ vars.base_path }}.context.json", "r") as f:
          context = json.load(f)
      
      # Run integrity scan
      result = scan_resume_integrity(context["resume_path"])
      
      # Save result
      with open("{{ vars.base_path }}.integrity.json", "w") as f:
          json.dump(result, f, indent=2)
      
      print(json.dumps(result))
    
    dependsOn:
      - prepare-context

  - id: code-quality-agent
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./agents:/agents
    script: |
      import json
      import sys
      sys.path.append("/agents")
      
      from code_quality import scan_code_quality
      
      # Sample code analysis (in real scenario, fetch from GitHub)
      code_sample = "function example() { const password = '123'; return password; }"
      
      # Run code quality scan
      result = scan_code_quality(code_sample)
      
      # Save result
      with open("{{ vars.base_path }}.code_quality.json", "w") as f:
          json.dump(result, f, indent=2)
      
      print(json.dumps(result))
    
    dependsOn:
      - prepare-context

  - id: uniqueness-agent
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./agents:/agents
    script: |
      import json
      import sys
      sys.path.append("/agents")
      
      from uniqueness import analyze_project_uniqueness
      
      # Load context
      with open("{{ vars.base_path }}.context.json", "r") as f:
          context = json.load(f)
      
      github_url = context.get("github_url")
      if github_url:
          result = analyze_project_uniqueness(github_url)
      else:
          result = {
              "agent": "uniqueness",
              "score": 5.0,
              "reasoning": "No GitHub URL provided for uniqueness analysis"
          }
      
      # Save result
      with open("{{ vars.base_path }}.uniqueness.json", "w") as f:
          json.dump(result, f, indent=2)
      
      print(json.dumps(result))
    
    dependsOn:
      - prepare-context

  - id: relevance-agent
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./agents:/agents
    script: |
      import json
      import sys
      sys.path.append("/agents")
      
      from relevance import evaluate_job_relevance
      
      # Load context
      with open("{{ vars.base_path }}.context.json", "r") as f:
          context = json.load(f)
      
      # Run relevance analysis
      result = evaluate_job_relevance(
          context["resume_text"], 
          context["job_description"]
      )
      
      # Save result
      with open("{{ vars.base_path }}.relevance.json", "w") as f:
          json.dump(result, f, indent=2)
      
      print(json.dumps(result))
    
    dependsOn:
      - prepare-context

  # Aggregate and synthesize results
  - id: final-evaluation
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
    script: |
      import json
      import os
      
      base_path = "{{ vars.base_path }}"
      
      # Load all agent results
      results = {}
      for agent in ["integrity", "code_quality", "uniqueness", "relevance"]:
          file_path = f"{base_path}.{agent}.json"
          if os.path.exists(file_path):
              with open(file_path, "r") as f:
                  results[agent] = json.load(f)
      
      # Extract and normalize scores
      integrity_score = results.get("integrity", {}).get("score", 0)
      quality_score = results.get("code_quality", {}).get("score", 50) / 10  # Convert to 0-10
      uniqueness_score = results.get("uniqueness", {}).get("score", 5)
      relevance_score = results.get("relevance", {}).get("score", 5)
      
      # Weighted scoring
      weights = {
          "integrity": 0.20,
          "quality": 0.30,
          "uniqueness": 0.30,
          "relevance": 0.20
      }
      
      overall_score = (
          integrity_score * weights["integrity"] +
          quality_score * weights["quality"] +
          uniqueness_score * weights["uniqueness"] +
          relevance_score * weights["relevance"]
      )
      
      # Generate recommendation
      if overall_score >= 7.0 and integrity_score >= 6.0:
          recommendation = "PASS"
          reasoning = f"Strong candidate with overall score of {overall_score:.1f}/10"
      elif overall_score >= 5.0 and integrity_score >= 4.0:
          recommendation = "WAITLIST"
          reasoning = f"Potential candidate with overall score of {overall_score:.1f}/10"
      else:
          recommendation = "REJECT"
          reasoning = f"Does not meet standards with overall score of {overall_score:.1f}/10"
      
      # Create final result
      final_result = {
          "candidate": {
              "resume_path": "{{ inputs.resume_path }}",
              "job_description": "{{ inputs.job_description }}",
              "github_url": "{{ inputs.github_url }}",
              "leetcode_username": "{{ inputs.leetcode_username }}"
          },
          "agents": results,
          "final": {
              "overall_score": round(overall_score, 1),
              "recommendation": recommendation,
              "reasoning": reasoning,
              "score_breakdown": {
                  "integrity": integrity_score,
                  "code_quality": quality_score,
                  "uniqueness": uniqueness_score,
                  "relevance": relevance_score
              },
              "evaluation_metadata": {
                  "model_backend": "hybrid_quantized",
                  "weights_used": weights,
                  "agents_executed": list(results.keys())
              }
          }
      }
      
      # Save final result
      with open(f"{base_path}.evaluation.json", "w") as f:
          json.dump(final_result, f, indent=2)
      
      print(json.dumps(final_result, indent=2))
    
    dependsOn:
      - integrity-agent
      - code-quality-agent
      - uniqueness-agent
      - relevance-agent

outputs:
  - id: evaluation-result
    type: JSON
    value: "{{ outputs.final-evaluation }}"