id: candidate-evaluation-enhanced
namespace: candidateai

description: |
  Parallel candidate evaluation with Kestra orchestration + Local AI models
  Enhanced with Ollama integration for improved accuracy

inputs:
  - name: resume_path
    type: STRING
    description: "Path to resume PDF file"
  - name: job_description
    type: STRING  
    description: "Job description text"
  - name: github_url
    type: STRING
    description: "GitHub repository URL (optional)"
    required: false
  - name: leetcode_username
    type: STRING
    description: "LeetCode username (optional)"
    required: false
  - name: use_ai_models
    type: BOOLEAN
    description: "Use local AI models (Ollama) if available"
    required: false
    default: true

variables:
  base_path: "{{ inputs.resume_path | replace('.pdf', '') }}"

tasks:
  # Prepare context and extract data
  - id: prepare-data
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./models:/models
    inputFiles:
      - pymupdf: requirements.txt
    script: |
      import json
      import os
      import sys
      
      # Install PDF processing
      os.system("pip install pymupdf requests psutil")
      
      import pymupdf as fitz
      import requests
      
      resume_path = "{{ inputs.resume_path }}"
      base_path = "{{ inputs.resume_path }}".replace('.pdf', '')
      
      # Extract text from resume
      try:
          doc = fitz.open(resume_path)
          text = ""
          for page in doc:
              text += page.get_text()
          resume_text = text[:2000]  # Limit for processing
          print(f"Extracted {len(text)} characters from resume")
      except Exception as e:
          print(f"Resume extraction failed: {e}")
          resume_text = f"Error extracting resume: {e}"
      
      # Check if Ollama is available
      ollama_available = False
      try:
          response = requests.get("http://host.docker.internal:11434/api/tags", timeout=2)
          ollama_available = response.status_code == 200
      except:
          print("Ollama not available - using heuristics")
      
      # Prepare context for agents
      context = {
          "resume_path": resume_path,
          "resume_text": resume_text,
          "job_description": "{{ inputs.job_description }}",
          "github_url": "{{ inputs.github_url }}",
          "leetcode_username": "{{ inputs.leetcode_username }}",
          "base_path": base_path,
          "ollama_available": ollama_available,
          "use_ai_models": "{{ inputs.use_ai_models }}"
      }
      
      # Save context
      with open(f"{base_path}.context.json", "w") as f:
          json.dump(context, f, indent=2)
      
      print(json.dumps({
          "status": "prepared", 
          "context_saved": f"{base_path}.context.json",
          "ollama_available": ollama_available,
          "use_ai": "{{ inputs.use_ai_models }}"
      }))

  # Enhanced Parallel agent execution
  - id: integrity-scan-enhanced
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./models:/models
    inputFiles:
      - pymupdf: requirements.txt
    script: |
      import json
      import sys
      import os
      sys.path.append('/app/agents')
      
      base_path = "{{ vars.base_path }}"
      use_ai = "{{ inputs.use_ai_models }}"
      
      # Load context
      with open(f"{base_path}.context.json", "r") as f:
          context = json.load(f)
      
      # Enhanced integrity scan with AI
      from integrity_enhanced import scan_resume_integrity
      
      print(f"Starting enhanced integrity scan (AI: {use_ai})")
      result = scan_resume_integrity(
          resume_path=context["resume_path"],
          use_ai_models=use_ai,
          ollama_available=context["ollama_available"]
      )
      
      # Save result
      with open(f"{base_path}.integrity.json", "w") as f:
          json.dump(result, f, indent=2)
      
      print(json.dumps(result))
    
    dependsOn:
      - prepare-data

  - id: code-quality-scan-enhanced
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./models:/models
    inputFiles:
      - pymupdf: requirements.txt
    script: |
      import json
      import sys
      import os
      sys.path.append('/app/agents')
      
      base_path = "{{ vars.base_path }}"
      use_ai = "{{ inputs.use_ai_models }}"
      
      # Load context
      with open(f"{base_path}.context.json", "r") as f:
          context = json.load(f)
      
      # Enhanced code quality scan with AI
      from code_quality_enhanced import scan_code_quality
      
      # Get sample code or fetch from GitHub if available
      code_sample = "function analyze() { /* sample code */ }"
      if context.get("github_url") and context.get("ollama_available"):
          try:
              import requests
              # Parse GitHub URL
              repo_path = context["github_url"].split("github.com/")[-1]
              api_url = f"https://api.github.com/repos/{repo_path}/contents/main.py"
              response = requests.get(api_url, timeout=10)
              if response.status_code == 200:
                  import base64
                  code_data = response.json()
                  code_sample = base64.b64decode(code_data.get("content", ""))
          except:
              pass
      
      print(f"Starting enhanced code quality scan (AI: {use_ai})")
      result = scan_code_quality(
          code_sample=code_sample,
          use_ai_models=use_ai,
          ollama_available=context["ollama_available"]
      )
      
      # Save result
      with open(f"{base_path}.code_quality.json", "w") as f:
          json.dump(result, f, indent=2)
      
      print(json.dumps(result))
    
    dependsOn:
      - prepare-data

  - id: uniqueness-analysis-enhanced
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./models:/models
    inputFiles:
      - pymupdf: requirements.txt
    script: |
      import json
      import sys
      import os
      sys.path.append('/app/agents')
      
      base_path = "{{ vars.base_path }}"
      use_ai = "{{ inputs.use_ai_models }}"
      
      # Load context
      with open(f"{base_path}.context.json", "r") as f:
          context = json.load(f)
      
      # Enhanced uniqueness analysis with AI
      from uniqueness_enhanced import analyze_project_uniqueness
      
      print(f"Starting enhanced uniqueness analysis (AI: {use_ai})")
      result = analyze_project_uniqueness(
          repo_url=context.get("github_url", ""),
          use_ai_models=use_ai,
          ollama_available=context["ollama_available"]
      )
      
      # Save result
      with open(f"{base_path}.uniqueness.json", "w") as f:
          json.dump(result, f, indent=2)
      
      print(json.dumps(result))
    
    dependsOn:
      - prepare-data

  - id: relevance-analysis-enhanced
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./models:/models
    inputFiles:
      - pymupdf: requirements.txt
    script: |
      import json
      import sys
      import os
      sys.path.append('/app/agents')
      
      base_path = "{{ vars.base_path }}"
      use_ai = "{{ inputs.use_ai_models }}"
      
      # Load context
      with open(f"{base_path}.context.json", "r") as f:
          context = json.load(f)
      
      # Enhanced relevance analysis with AI
      from relevance_enhanced import evaluate_job_relevance
      
      print(f"Starting enhanced relevance analysis (AI: {use_ai})")
      result = evaluate_job_relevance(
          resume_text=context["resume_text"],
          job_description=context["job_description"],
          use_ai_models=use_ai,
          ollama_available=context["ollama_available"]
      )
      
      # Save result
      with open(f"{base_path}.relevance.json", "w") as f:
          json.dump(result, f, indent=2)
      
      print(json.dumps(result))
    
    dependsOn:
      - prepare-data

  # Enhanced result aggregation with AI synthesis
  - id: enhanced-synthesis
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: python:3.9-slim
    volumes:
      - ./data:/data
      - ./models:/models
    inputFiles:
      - pymupdf: requirements.txt
    script: |
      import json
      import sys
      import os
      
      base_path = "{{ vars.base_path }}"
      use_ai = "{{ inputs.use_ai_models }}"
      
      # Load all agent results
      results = {}
      for agent in ["integrity", "code_quality", "uniqueness", "relevance"]:
          file_path = f"{base_path}.{agent}.json"
          if os.path.exists(file_path):
              with open(file_path, "r") as f:
                  results[agent] = json.load(f)
      
      # Calculate scores with proper scaling
      integrity_score = results.get("integrity", {}).get("score", 0)
      quality_score = results.get("code_quality", {}).get("score", 50) / 10  # Convert to 0-10 scale
      uniqueness_score = results.get("uniqueness", {}).get("score", 5)
      relevance_score = results.get("relevance", {}).get("score", 5)
      
      # Weighted scoring
      weights = {"integrity": 0.20, "quality": 0.30, "uniqueness": 0.30, "relevance": 0.20}
      
      overall_score = (
          integrity_score * weights["integrity"] +
          quality_score * weights["quality"] +
          uniqueness_score * weights["uniqueness"] +
          relevance_score * weights["relevance"]
      )
      
      # Generate recommendation
      if overall_score >= 7.0 and integrity_score >= 6.0:
          recommendation = "PASS"
          reasoning = f"Strong candidate with overall score of {overall_score:.1f}/10"
      elif overall_score >= 5.0 and integrity_score >= 4.0:
          recommendation = "WAITLIST"
          reasoning = f"Potential candidate with overall score of {overall_score:.1f}/10"
      else:
          recommendation = "REJECT"
          reasoning = f"Does not meet standards with overall score of {overall_score:.1f}/10"
      
      # Enhanced AI synthesis if available
      ai_summary = "Basic heuristic summary"
      if use_ai and results.get("prepare-data", {}).get("ollama_available"):
          try:
              import requests
              
              synthesis_prompt = f"""Based on these candidate evaluation results, provide a professional hiring recommendation in 2 sentences:
              
              Results: {json.dumps(results, indent=2)}
              Overall Score: {overall_score:.1f}/10
              Recommendation: {recommendation}
              
              Focus on actionable insights for hiring decision."""
              
              response = requests.post(
                  "http://host.docker.internal:11434/api/generate",
                  json={
                      "model": "qwen2:1.5b",
                      "prompt": synthesis_prompt,
                      "stream": False,
                      "options": {"temperature": 0.3, "num_predict": 200}
                  },
                  timeout=30
              )
              
              if response.status_code == 200:
                  result = response.json()
                  ai_summary = result.get("response", ai_summary)
          except:
              pass
      else:
          print("AI synthesis not available - using heuristics")
      
      final_result = {
          "candidate": {
              "resume_path": "{{ inputs.resume_path }}",
              "job_description": "{{ inputs.job_description }}",
              "github_url": "{{ inputs.github_url }}",
              "leetcode_username": "{{ inputs.leetcode_username }}"
          },
          "agents": results,
          "final": {
              "overall_score": round(overall_score, 1),
              "recommendation": recommendation,
              "reasoning": reasoning,
              "ai_summary": ai_summary,
              "score_breakdown": {
                  "integrity": integrity_score,
                  "code_quality": quality_score,
                  "uniqueness": uniqueness_score,
                  "relevance": relevance_score
              },
              "evaluation_metadata": {
                  "model_backend": "enhanced" if use_ai else "heuristics",
                  "weights_used": weights,
                  "agents_executed": list(results.keys()),
                  "ai_synthesis_enabled": use_ai,
                  "ai_synthesis_available": results.get("prepare-data", {}).get("ollama_available", False)
              }
          }
      }
      
      # Save final result
      with open(f"{base_path}.evaluation.json", "w") as f:
          json.dump(final_result, f, indent=2)
      
      print(json.dumps(final_result, indent=2))
    
    dependsOn:
      - integrity-scan-enhanced
      - code-quality-scan-enhanced
      - uniqueness-analysis-enhanced
      - relevance-analysis-enhanced

outputs:
  - id: enhanced-evaluation-result
    type: JSON
    value: "{{ outputs.enhanced-synthesis }}"