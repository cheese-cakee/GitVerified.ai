id: gitverified_pipeline
namespace: ai.realengineers
description: "The 100% Honest Resume Filter Pipeline"

inputs:
  - name: batch_id
    type: STRING
    defaults: "batch_001"
  - name: resumes_dir
    type: STRING
    defaults: "c:/Users/lenovo/RealEngineers.ai/uploads"

tasks:
  - id: list_resumes
    type: io.kestra.plugin.core.filesystem.List
    path: "{{ inputs.resumes_dir }}"
    regexp: ".*\\.pdf"

  - id: process_batch
    type: io.kestra.plugin.core.flow.ForEach
    values: "{{ outputs.list_resumes.files }}"
    concurrency_limit: 5 # Parallel agents
    tasks:
      - id: analyze_single_candidate
        type: io.kestra.plugin.scripts.python.Script
        taskRunner:
          type: io.kestra.plugin.scripts.runner.docker.Docker
        containerImage: python:3.9
        inputFiles:
          detect_white_text.py: |
            import fitz  # PyMuPDF
            import sys
            import json

            def detect_white_text(pdf_path):
                # NOTE: In the Docker runner, we might need to ensure PyMuPDF is installed or use a base image with it.
                # For this hackathon demo, we will use a robust mock if libs are missing, or assume standard env.
                # Here we paste the logic from scripts/detect_white_text.py
                try:
                    doc = fitz.open(pdf_path)
                except:
                    # Fallback if fitz not installed in runner
                    print(json.dumps({"status": "CLEAN", "note": "PyMuPDF not found, skipping check"}))
                    return

                suspicious_content = []
                for page_num, page in enumerate(doc):
                    blocks = page.get_text("dict")["blocks"]
                    for b in blocks:
                        if "lines" in b:
                            for line in b["lines"]:
                                for span in line["spans"]:
                                    color = span["color"]
                                    text = span["text"].strip()
                                    if color == 16777215 and len(text) > 0:
                                        suspicious_content.append({"page": page_num + 1, "text": text, "type": "white_text_exact"})
                                    if span["size"] < 1.0 and len(text) > 0:
                                         suspicious_content.append({"page": page_num + 1, "text": text, "type": "tiny_font"})

                if suspicious_content:
                    print(json.dumps({"status": "CHEATER_DETECTED", "details": suspicious_content}))
                    # We don't exit(1) here to allow the flow to record the cheater status rather than crash
                else:
                    print(json.dumps({"status": "CLEAN"}))

            if __name__ == "__main__":
                if len(sys.argv) > 1: detect_white_text(sys.argv[1])
                else: print(json.dumps({"status": "CLEAN"}))

          extract_links.py: |
            import fitz  # PyMuPDF
            import re
            import sys
            import json

            def extract_links(pdf_path):
                """
                Extracts GitHub and LeetCode URLs from a PDF file.
                """
                try:
                    doc = fitz.open(pdf_path)
                except Exception as e:
                    print(json.dumps({"error": f"Failed to open PDF: {str(e)}"}))
                    return

                text_content = ""
                for page in doc:
                    text_content += page.get_text()

                # Regex patterns
                github_pattern = r"(https?://)?(www\.)?github\.com/[a-zA-Z0-9_-]+(/[a-zA-Z0-9._-]+)*"
                leetcode_pattern = r"(https?://)?(www\.)?leetcode\.com/(u/)?[a-zA-Z0-9_-]+"

                github_urls = [m.group(0) for m in re.finditer(github_pattern, text_content)]
                leetcode_urls = [m.group(0) for m in re.finditer(leetcode_pattern, text_content)]

                result = {
                    "file": pdf_path,
                    "github_links": list(set(github_urls)),
                    "leetcode_links": list(set(leetcode_urls)),
                    "raw_text_snippet": text_content[:200]
                }
                
                print(json.dumps(result, indent=2))

            if __name__ == "__main__":
                if len(sys.argv) < 2:
                    print(json.dumps({"error": "No file provided"}))
                else:
                    extract_links(sys.argv[1])

          agents.py: |
            import json
            import random
            import sys
            # Emulating agents/oumi_agent.py and agents/coderabbit_check.py

            class OumiResumeJudge:
                def analyze(self, resume_text, github_url=None):
                    # Mocking the DPO model logic
                    score_truth = 100
                    score_passion = 50
                    reason = "Standard assessment."
                    status = "INTERVIEW"
                    lower_text = resume_text.lower()
                    
                    if "expert" in lower_text and "junior" in lower_text:
                         score_truth = 20; reason = "REJECT. Claiming 'Expert' while 'Junior'."
                    elif not github_url:
                         score_truth = 40; reason = "REJECT. Low Proof-of-Work."
                    elif "white text" in lower_text:
                         score_truth = 0; reason = "BLACKLIST. White text artifacts."
                    
                    if "open source" in lower_text or "maintainer" in lower_text:
                        score_passion = 95; reason = "INTERVIEW. Verified open source contribution."
                    if "game engine" in lower_text:
                        score_passion = 98; reason = "INTERVIEW. High competence (Systems)."

                    return {"truth_score": score_truth, "passion_score": score_passion, "reasoning": reason, "decision": status}

            class CoderabbitAuditor:
                def audit_repo(self, repo_url):
                    if not repo_url or "github" not in repo_url:
                        return {"score": 0, "findings": ["No valid repo"]}
                    
                    score = 70
                    findings = []
                    if "school" in repo_url: score = 30; findings.append("Pattern: 'School Assignment'")
                    elif "engine" in repo_url: score = 95; findings.append("High Complexity: Systems detected")
                    elif "bot" in repo_url: score = 60; findings.append("Automation Script")
                    else: score = 85; findings.append("Standard project")
                    
                    return {"code_quality_score": score, "findings": findings}

            class LeetCodeScorer:
                def score(self, leetcode_url):
                    if not leetcode_url or "leetcode" not in leetcode_url:
                        return {"score": 0, "rank": "None"}
                    
                    # Mock Logic for Hackathon
                    # In real life, we would scrape the profile for "Solved Count"
                    mock_solved = random.randint(0, 500)
                    rating = 0
                    if mock_solved > 200: rating = 90
                    elif mock_solved > 50: rating = 70
                    else: rating = 40
                    
                    return {"score": rating, "solved": mock_solved}

            if __name__ == "__main__":
                # Load extracted links
                try:
                    with open("links.json", "r") as f:
                        links_data = json.load(f)
                        github_links = links_data.get("github_links", [])
                        leetcode_links = links_data.get("leetcode_links", [])
                        resume_text = links_data.get("raw_text_snippet", "")
                        
                        github_url = github_links[0] if github_links else None
                        leetcode_url = leetcode_links[0] if leetcode_links else None
                except:
                    resume_text = "Experienced Junior Expert. Github: github.com/user/game-engine"
                    github_url = "github.com/user/game-engine"
                    leetcode_url = None

                # Check White Text Result
                is_cheater = False
                try:
                    with open("whitetext.json", "r") as f:
                        wt_data = json.load(f)
                        if wt_data.get("status") == "CHEATER_DETECTED":
                            is_cheater = True
                except:
                    pass
                
                oumi = OumiResumeJudge()
                coderabbit = CoderabbitAuditor()
                leetcode = LeetCodeScorer()
                
                oumi_res = oumi.analyze(resume_text, github_url)
                
                # OVERRIDE OUMI IF CHEATER
                if is_cheater:
                    oumi_res["truth_score"] = 0
                    oumi_res["reasoning"] = "BLACKLIST. White Text Injection Detected."
                    oumi_res["decision"] = "REJECT"

                code_res = coderabbit.audit_repo(github_url)
                lc_res = leetcode.score(leetcode_url)
                
                # Weighted P-Score Calculation
                # Truth (Oumi) = 40%
                # Code (Coderabbit) = 40%
                # Passion/Leetcode = 20%
                
                p_score = (oumi_res["truth_score"] * 0.4) + \
                          (code_res["code_quality_score"] * 0.4) + \
                          (oumi_res["passion_score"] * 0.1) + \
                          (lc_res["score"] * 0.1)
                
                full_result = {
                    "candidate_id": sys.argv[1] if len(sys.argv) > 1 else "unknown",
                    "oumi_check": oumi_res,
                    "coderabbit_check": code_res,
                    "leetcode_check": lc_res,
                    "final_p_score": int(p_score),
                    "final_status": "INTERVIEW" if p_score > 70 else "REJECT"
                }
                print(json.dumps(full_result))

        script: |
          # 1. Check for White Text Cheats
          python detect_white_text.py "{{ taskrun.value }}" > whitetext.json

          # 2. Extract Links
          python extract_links.py "{{ taskrun.value }}" > links.json

          # 3. Run Agents
          python agents.py "{{ taskrun.value }}" > result.json
          cat result.json

  - id: aggregate_results
    type: io.kestra.plugin.scripts.python.Script
    script: |
      print("Aggregating leaderboard...")
      # Logic to read all result.json files and sort them
      print("Batch processing complete.")
