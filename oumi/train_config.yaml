# Oumi DPO Training Configuration
# Target: "Iron Intelligence" Prize
# Base Model: Llama-3-8B-Instruct (or Qwen-2-7B-Instruct)

model:
  name: "meta-llama/Meta-Llama-3-8B-Instruct" 
  # Using LoRA for efficient fine-tuning on consumer/hackathon hardware
  adapter_method: "lora"
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "v_proj"]

data:
  train:
    datasets:
      - type: "jsonl"
        path: "c:/Users/lenovo/RealEngineers.ai/data/resume_judge_dpo.jsonl"
  
training:
  method: "dpo"  # Direct Preference Optimization
  batch_size: 1
  gradient_accumulation_steps: 8 # Increased for better stability
  learning_rate: 3e-6 # Slightly lower for fine-grained convergence
  beta: 0.1  # DPO beta parameter
  num_epochs: 10 # "Train it a lot more"
  warmup_ratio: 0.1
  optimizer: "adamw_torch"
  save_steps: 50
  logging_steps: 1
  output_dir: "c:/Users/lenovo/RealEngineers.ai/oumi/output/gitverified-dpo"

# "Iron Intelligence" specific settings
# We want the model to be strict and analytical
generation:
  max_new_tokens: 512
  temperature: 0.7 
  top_p: 0.9

environment:
  use_fp16: true
  device: "cuda" # Assumes GPU availability
