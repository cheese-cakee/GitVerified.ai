# CandidateAI - Hybrid Architecture

## üéØ Optimal Architecture: Production + Local

### Keep What's Valuable
- ‚úÖ **Kestra** - Workflow orchestration, parallel execution
- ‚úÖ **Docker** - Consistent deployment, scaling
- ‚úÖ **Oumi** - But with quantization for efficiency

### Optimize What's Heavy
- üîÑ **Quantized Models** - 4-bit Oumi models (4GB vs 16GB)
- üîÑ **Smart Caching** - Local model downloads
- üîÑ **Selective Docker** - Only for orchestration, not agents

## üèóÔ∏è Final Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Frontend                    ‚îÇ
‚îÇ         (Next.js - Optional)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 Kestra                   ‚îÇ
‚îÇ         (Dockerized Orchestrator)        ‚îÇ
‚îÇ  - Parallel agent execution            ‚îÇ
‚îÇ  - Error handling & retries            ‚îÇ
‚îÇ  - Web UI for monitoring              ‚îÇ
‚îÇ  - Scalable workflow management        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Local Agents                ‚îÇ
‚îÇ    (Python scripts in containers)        ‚îÇ
‚îÇ                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Integrity  ‚îÇ ‚îÇ   Code Quality  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Scanner   ‚îÇ ‚îÇ   Analyzer      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Uniqueness  ‚îÇ ‚îÇ    Relevance    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Judge    ‚îÇ ‚îÇ   Evaluator     ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚ñº              ‚îÇ
‚îÇ               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ               ‚îÇ   Quantized     ‚îÇ        ‚îÇ
‚îÇ               ‚îÇ    Oumi         ‚îÇ        ‚îÇ
‚îÇ               ‚îÇ (4-bit models)  ‚îÇ        ‚îÇ
‚îÇ               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üíæ Memory Optimization

### Heavy Setup (Before)
```
Models:
- Llama-3-8B: 16GB RAM
- Docker services: 4GB RAM  
- Kestra: 2GB RAM
Total: ~22GB RAM required
```

### Optimized Setup (After)  
```
Models:
- Llama-3-8B-4bit: 4GB RAM
- Qwen2-1.5B: 1GB RAM (fallback)
- Docker services: 2GB RAM
- Kestra: 1GB RAM
Total: ~8GB RAM (works on laptops!)
```

## üîß Implementation Strategy

### 1. Lightweight Oumi Configuration
```python
# agents/oumi_client.py
from oumi.inference import LlamaInferenceEngine
from oumi.core.configs import ModelParams

def get_lightweight_engine():
    return LlamaInferenceEngine(
        model_params=ModelParams(
            model_name="meta-llama/Llama-3-8B-Instruct",
            model_kwargs={
                "load_in_4bit": True,
                "device_map": "auto"
            }
        )
    )
```

### 2. Optimized Kestra Pipeline
```yaml
# flows/candidate_evaluation.yaml
id: candidate-evaluation
namespace: candidateai

tasks:
  - id: evaluate-candidate
    type: io.kestra.plugin.scripts.python.Script
    runner: DOCKER
    docker:
      image: candidateai/agents:latest
      volumes:
        - /models:/models
    inputFiles:
      - requirements.txt
    script: |
      from agents.evaluator import CandidateEvaluator
      evaluator = CandidateEvaluator()
      results = evaluator.evaluate_all(inputs)
      print(json.dumps(results))
```

### 3. Multi-Model Strategy
```python
# agents/model_selector.py
def select_model(available_memory_gb):
    if available_memory_gb >= 12:
        return "meta-llama/Llama-3-8B-Instruct-4bit"
    elif available_memory_gb >= 6:
        return "Qwen/Qwen2-7B-Instruct-4bit"  
    else:
        return "Qwen/Qwen2-1.5B-Instruct"  # No quantization needed
```

## üìä Performance Comparison

| Setup | RAM | Setup Time | Accuracy | Production Ready |
|-------|-----|------------|----------|------------------|
| API-only | 2GB | 5 min | 95% | ‚ùå Expensive |
| Local-only | 4GB | 10 min | 75% | ‚ùå Not scalable |
| **Hybrid** | **8GB** | **15 min** | **85%** | **‚úÖ Perfect** |

## üöÄ Benefits of Hybrid Approach

### Production Features (from Kestra/Docker)
- **Parallel execution** - 5x faster evaluations
- **Error handling** - Automatic retries and logging
- **Web monitoring** - Visual workflow tracking
- **Scalability** - Multiple candidates simultaneously
- **Easy deployment** - One command setup

### Local Benefits (from quantized Oumi)
- **Privacy** - Models run locally
- **Cost** - No API charges
- **Speed** - Local inference (no network latency)
- **Offline** - Works without internet
- **Control** - Custom fine-tuning possible

---

**This hybrid approach gives you enterprise-grade features with the privacy and cost benefits of local AI.**