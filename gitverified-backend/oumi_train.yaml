# Oumi DPO Training Configuration
# Fine-tunes Llama 3 to distinguish tutorial clones from novel engineering
# Uses Direct Preference Optimization (DPO)

model:
  name: "meta-llama/Meta-Llama-3-8B-Instruct"
  load_in_8bit: true

data:
  train:
    datasets:
      - name: "gitverified/junior-vs-senior-v1"
        split: "train"
        # Features: "bad_code" (Weather App) vs "good_code" (Custom Kernel)

training:
  method: "dpo" # Direct Preference Optimization (Oumi Supported)
  learning_rate: 5e-7
  batch_size: 2
  gradient_accumulation_steps: 4
  num_epochs: 3

  # RL Specifics
  beta: 0.1 # DPO Beta

output_dir: "./models/gitverified-judge-v1"
# Training configuration for fine-tuning the uniqueness judge model
